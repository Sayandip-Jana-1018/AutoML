import { NextRequest, NextResponse } from 'next/server';
import { adminAuth } from '@/lib/firebase-admin';

/**
 * Export training configuration as Jupyter Notebook
 * POST /api/export/notebook
 */
export async function POST(req: NextRequest) {
    try {
        // Verify auth
        const authHeader = req.headers.get('authorization');
        if (!authHeader?.startsWith('Bearer ')) {
            return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
        }

        const token = authHeader.split('Bearer ')[1];
        await adminAuth.verifyIdToken(token);

        const body = await req.json();
        const {
            projectName,
            algorithm,
            targetColumn,
            featureColumns,
            testSize,
            epochs,
            learningRate,
            trainScript
        } = body;

        // Generate Jupyter Notebook JSON
        const notebook = {
            "nbformat": 4,
            "nbformat_minor": 5,
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                },
                "language_info": {
                    "name": "python",
                    "version": "3.9.0"
                },
                "colab": {
                    "name": `MLForge - ${projectName || 'Model Training'}`,
                    "provenance": []
                }
            },
            "cells": [
                // Title Cell
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        `# ðŸš€ ${projectName || 'ML Model'} - Training Notebook\n`,
                        `\n`,
                        `*Generated by MLForge Studio*\n`,
                        `\n`,
                        `## Configuration\n`,
                        `| Parameter | Value |\n`,
                        `|-----------|-------|\n`,
                        `| **Algorithm** | ${algorithm || 'Auto-selected'} |\n`,
                        `| **Target** | ${targetColumn || 'N/A'} |\n`,
                        `| **Features** | ${featureColumns?.length || 0} columns |\n`,
                        `| **Test Size** | ${testSize || 0.2} |\n`
                    ]
                },
                // Setup Cell
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": [
                        "# Install dependencies\n",
                        "!pip install -q pandas scikit-learn numpy matplotlib seaborn\n"
                    ],
                    "execution_count": null,
                    "outputs": []
                },
                // Imports Cell
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": [
                        "# Imports\n",
                        "import pandas as pd\n",
                        "import numpy as np\n",
                        "import matplotlib.pyplot as plt\n",
                        "import seaborn as sns\n",
                        "from sklearn.model_selection import train_test_split\n",
                        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                        "import warnings\n",
                        "warnings.filterwarnings('ignore')\n",
                        "\n",
                        "print('âœ… Libraries loaded!')\n"
                    ],
                    "execution_count": null,
                    "outputs": []
                },
                // Data Loading Cell
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": [
                        "# Load your dataset\n",
                        "# Replace with your actual data path\n",
                        "# from google.colab import files\n",
                        "# uploaded = files.upload()\n",
                        "\n",
                        "# For demo, we'll use a sample path\n",
                        "DATA_PATH = 'your_dataset.csv'\n",
                        "\n",
                        "try:\n",
                        "    df = pd.read_csv(DATA_PATH)\n",
                        "    print(f'âœ… Loaded {len(df)} rows, {len(df.columns)} columns')\n",
                        "    df.head()\n",
                        "except:\n",
                        "    print('âš ï¸ Please upload your dataset first')\n"
                    ],
                    "execution_count": null,
                    "outputs": []
                },
                // Training Script Cell (if provided)
                ...(trainScript ? [{
                    "cell_type": "code",
                    "metadata": {},
                    "source": trainScript.split('\n').map((line: string) => line + '\n'),
                    "execution_count": null,
                    "outputs": []
                }] : [
                    // Default training code if no script provided
                    {
                        "cell_type": "code",
                        "metadata": {},
                        "source": [
                            `# Target and Features\n`,
                            `TARGET = '${targetColumn || "target"}'\n`,
                            `FEATURES = ${JSON.stringify(featureColumns || [])}\n`,
                            `\n`,
                            `# If no features specified, use all except target\n`,
                            `if not FEATURES:\n`,
                            `    FEATURES = [col for col in df.columns if col != TARGET]\n`,
                            `\n`,
                            `X = df[FEATURES]\n`,
                            `y = df[TARGET]\n`,
                            `\n`,
                            `# Handle categorical variables\n`,
                            `X = pd.get_dummies(X, drop_first=True)\n`,
                            `\n`,
                            `# Split data\n`,
                            `X_train, X_test, y_train, y_test = train_test_split(\n`,
                            `    X, y, test_size=${testSize || 0.2}, random_state=42\n`,
                            `)\n`,
                            `\n`,
                            `print(f'Training set: {len(X_train)} samples')\n`,
                            `print(f'Test set: {len(X_test)} samples')\n`
                        ],
                        "execution_count": null,
                        "outputs": []
                    },
                    {
                        "cell_type": "code",
                        "metadata": {},
                        "source": getModelCode(algorithm, epochs, learningRate),
                        "execution_count": null,
                        "outputs": []
                    }
                ]),
                // Evaluation Cell
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": [
                        "# Evaluate the model\n",
                        "y_pred = model.predict(X_test)\n",
                        "\n",
                        "# Accuracy\n",
                        "accuracy = accuracy_score(y_test, y_pred)\n",
                        "print(f'\\nðŸŽ¯ Accuracy: {accuracy:.2%}')\n",
                        "\n",
                        "# Classification Report\n",
                        "print('\\nðŸ“Š Classification Report:')\n",
                        "print(classification_report(y_test, y_pred))\n"
                    ],
                    "execution_count": null,
                    "outputs": []
                },
                // Visualization Cell
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": [
                        "# Confusion Matrix Visualization\n",
                        "plt.figure(figsize=(8, 6))\n",
                        "cm = confusion_matrix(y_test, y_pred)\n",
                        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                        "plt.title('Confusion Matrix')\n",
                        "plt.ylabel('Actual')\n",
                        "plt.xlabel('Predicted')\n",
                        "plt.show()\n"
                    ],
                    "execution_count": null,
                    "outputs": []
                },
                // Feature Importance Cell
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": [
                        "# Feature Importance (if available)\n",
                        "try:\n",
                        "    if hasattr(model, 'feature_importances_'):\n",
                        "        importances = pd.DataFrame({\n",
                        "            'feature': X.columns,\n",
                        "            'importance': model.feature_importances_\n",
                        "        }).sort_values('importance', ascending=True).tail(15)\n",
                        "        \n",
                        "        plt.figure(figsize=(10, 8))\n",
                        "        plt.barh(importances['feature'], importances['importance'])\n",
                        "        plt.title('Top 15 Feature Importances')\n",
                        "        plt.xlabel('Importance')\n",
                        "        plt.show()\n",
                        "except:\n",
                        "    print('Feature importance not available for this model')\n"
                    ],
                    "execution_count": null,
                    "outputs": []
                },
                // Save Model Cell
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": [
                        "# Save the trained model\n",
                        "import joblib\n",
                        "\n",
                        "MODEL_PATH = 'trained_model.joblib'\n",
                        "joblib.dump(model, MODEL_PATH)\n",
                        "print(f'âœ… Model saved to {MODEL_PATH}')\n",
                        "\n",
                        "# Download (in Colab)\n",
                        "# from google.colab import files\n",
                        "# files.download(MODEL_PATH)\n"
                    ],
                    "execution_count": null,
                    "outputs": []
                }
            ]
        };

        // Return as downloadable file
        return new NextResponse(JSON.stringify(notebook, null, 2), {
            status: 200,
            headers: {
                'Content-Type': 'application/x-ipynb+json',
                'Content-Disposition': `attachment; filename="${projectName || 'mlforge'}_training.ipynb"`
            }
        });

    } catch (error: any) {
        console.error('[Export Notebook] Error:', error);
        return NextResponse.json({ error: error.message }, { status: 500 });
    }
}

/**
 * Generate model-specific training code
 */
function getModelCode(algorithm: string | undefined, epochs: number | undefined, learningRate: number | undefined): string[] {
    const alg = (algorithm || 'random_forest').toLowerCase();

    if (alg.includes('random_forest') || alg.includes('rf')) {
        return [
            "# Random Forest Classifier\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            `model = RandomForestClassifier(\n`,
            `    n_estimators=${epochs || 100},\n`,
            `    max_depth=10,\n`,
            `    random_state=42,\n`,
            `    n_jobs=-1\n`,
            `)\n`,
            "\n",
            "model.fit(X_train, y_train)\n",
            "print('âœ… Model trained!')\n"
        ];
    }

    if (alg.includes('xgb') || alg.includes('gradient')) {
        return [
            "# XGBoost Classifier\n",
            "!pip install -q xgboost\n",
            "from xgboost import XGBClassifier\n",
            "\n",
            `model = XGBClassifier(\n`,
            `    n_estimators=${epochs || 100},\n`,
            `    learning_rate=${learningRate || 0.1},\n`,
            `    max_depth=6,\n`,
            `    random_state=42,\n`,
            `    use_label_encoder=False,\n`,
            `    eval_metric='logloss'\n`,
            `)\n`,
            "\n",
            "model.fit(X_train, y_train)\n",
            "print('âœ… Model trained!')\n"
        ];
    }

    if (alg.includes('logistic')) {
        return [
            "# Logistic Regression\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "# Scale features\n",
            "scaler = StandardScaler()\n",
            "X_train_scaled = scaler.fit_transform(X_train)\n",
            "X_test_scaled = scaler.transform(X_test)\n",
            "\n",
            `model = LogisticRegression(\n`,
            `    max_iter=${epochs || 1000},\n`,
            `    random_state=42\n`,
            `)\n`,
            "\n",
            "model.fit(X_train_scaled, y_train)\n",
            "X_test = X_test_scaled  # Use scaled for predictions\n",
            "print('âœ… Model trained!')\n"
        ];
    }

    // Default: Random Forest
    return [
        "# Auto-selected: Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print('âœ… Model trained!')\n"
    ];
}
