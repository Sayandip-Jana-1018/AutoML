/**
 * TFRecord Converter - Generates Python script for converting image folders to TFRecord shards
 * 
 * Features:
 * - Handles train/val/test splits
 * - Creates ~50MB shards for optimal GPU loading
 * - Stores metadata with class names, counts, dimensions
 * - Adds MD5 checksums for integrity verification
 */

export interface TFRecordMetadata {
    numClasses: number;
    classNames: string[];
    trainSamples: number;
    valSamples?: number;
    testSamples?: number;
    originalWidth?: number;
    originalHeight?: number;
    shardPaths: {
        train: string[];
        val?: string[];
        test?: string[];
    };
    checksums: Record<string, string>;
    createdAt: string;
}

export function generateTFRecordConverterScript(config: {
    inputDir: string;
    outputDir: string;
    shardSizeMB?: number;
    valSplit?: number;
}): string {
    const { inputDir, outputDir, shardSizeMB = 50, valSplit = 0.2 } = config;

    return `#!/usr/bin/env python3
"""
TFRecord Converter - Generated by MLForge
Converts image folder dataset to TFRecord shards for fast training.
"""

import tensorflow as tf
import os
import json
import hashlib
import random
from pathlib import Path
from PIL import Image
import io

# Configuration
INPUT_DIR = "${inputDir}"
OUTPUT_DIR = "${outputDir}"
SHARD_SIZE_MB = ${shardSizeMB}
VAL_SPLIT = ${valSplit}

def _bytes_feature(value):
    """Returns a bytes_list from a string / byte."""
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _int64_feature(value):
    """Returns an int64_list from a bool / enum / int / uint."""
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def find_image_folder(base_path, max_depth=4):
    """Find the folder containing train/class structure"""
    def get_subdirs(path):
        if not os.path.exists(path):
            return []
        return [d for d in os.listdir(path) 
                if os.path.isdir(os.path.join(path, d)) 
                and d not in ['__pycache__', '.git', '__MACOSX']]
    
    def has_images(folder):
        try:
            files = os.listdir(folder)[:20]
            return any(f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')) for f in files)
        except:
            return False
    
    def search(path, depth=0):
        if depth > max_depth:
            return None
        
        subdirs = get_subdirs(path)
        subdir_lower = {d.lower(): d for d in subdirs}
        
        # Check for train folder
        for name in ['train', 'training']:
            if name in subdir_lower:
                train_dir = os.path.join(path, subdir_lower[name])
                train_subdirs = get_subdirs(train_dir)
                if train_subdirs and has_images(os.path.join(train_dir, train_subdirs[0])):
                    test_dir = None
                    for t in ['test', 'val', 'validation']:
                        if t in subdir_lower:
                            test_dir = os.path.join(path, subdir_lower[t])
                            break
                    return {'train': train_dir, 'test': test_dir}
        
        # Check for direct class folders
        if len(subdirs) > 1:
            first_class = os.path.join(path, subdirs[0])
            if has_images(first_class):
                return {'train': path, 'test': None}
        
        # Recurse
        for subdir in subdirs:
            result = search(os.path.join(path, subdir), depth + 1)
            if result:
                return result
        
        return None
    
    return search(base_path)

def collect_images(class_dir, class_idx):
    """Collect all image paths from a class directory"""
    images = []
    extensions = ('.png', '.jpg', '.jpeg', '.webp', '.gif', '.bmp')
    
    for img_file in os.listdir(class_dir):
        if img_file.lower().endswith(extensions):
            img_path = os.path.join(class_dir, img_file)
            images.append((img_path, class_idx))
    
    return images

def get_image_dimensions(img_path):
    """Get original image dimensions"""
    try:
        with Image.open(img_path) as img:
            return img.size  # (width, height)
    except:
        return (0, 0)

def create_example(img_path, label):
    """Create a TFRecord example from an image"""
    with open(img_path, 'rb') as f:
        img_bytes = f.read()
    
    example = tf.train.Example(features=tf.train.Features(feature={
        'image': _bytes_feature(img_bytes),
        'label': _int64_feature(label),
        'filename': _bytes_feature(os.path.basename(img_path).encode()),
    }))
    
    return example.SerializeToString()

def write_shards(images, output_prefix, shard_size_bytes):
    """Write images to TFRecord shards"""
    shard_paths = []
    checksums = {}
    
    shard_idx = 0
    current_size = 0
    writer = None
    current_shard_path = None
    hasher = None
    
    for img_path, label in images:
        # Start new shard if needed
        if writer is None or current_size >= shard_size_bytes:
            if writer:
                writer.close()
                # Compute checksum for completed shard
                with open(current_shard_path, 'rb') as f:
                    checksums[os.path.basename(current_shard_path)] = hashlib.md5(f.read()).hexdigest()
            
            current_shard_path = f"{output_prefix}-{shard_idx:05d}.tfrecord"
            shard_paths.append(os.path.basename(current_shard_path))
            writer = tf.io.TFRecordWriter(current_shard_path)
            shard_idx += 1
            current_size = 0
            print(f"  Writing shard: {current_shard_path}")
        
        try:
            serialized = create_example(img_path, label)
            writer.write(serialized)
            current_size += len(serialized)
        except Exception as e:
            print(f"  Warning: Failed to process {img_path}: {e}")
    
    # Close final shard
    if writer:
        writer.close()
        with open(current_shard_path, 'rb') as f:
            checksums[os.path.basename(current_shard_path)] = hashlib.md5(f.read()).hexdigest()
    
    return shard_paths, checksums

def convert_dataset():
    """Main conversion function"""
    print("=" * 60)
    print("TFRecord Converter - MLForge")
    print("=" * 60)
    
    # Find dataset structure
    print(f"\\nSearching for dataset in: {INPUT_DIR}")
    folders = find_image_folder(INPUT_DIR)
    
    if not folders:
        raise ValueError(f"Could not find valid image dataset in {INPUT_DIR}")
    
    train_dir = folders['train']
    test_dir = folders.get('test')
    
    print(f"Found train folder: {train_dir}")
    if test_dir:
        print(f"Found test folder: {test_dir}")
    
    # Discover classes
    class_names = sorted([d for d in os.listdir(train_dir) 
                         if os.path.isdir(os.path.join(train_dir, d))
                         and d not in ['__pycache__', '__MACOSX']])
    class_to_idx = {name: idx for idx, name in enumerate(class_names)}
    
    print(f"\\nFound {len(class_names)} classes:")
    for i, name in enumerate(class_names[:10]):
        print(f"  {i}: {name}")
    if len(class_names) > 10:
        print(f"  ... and {len(class_names) - 10} more")
    
    # Collect all training images
    print("\\nCollecting training images...")
    train_images = []
    sample_img = None
    for class_name in class_names:
        class_dir = os.path.join(train_dir, class_name)
        imgs = collect_images(class_dir, class_to_idx[class_name])
        train_images.extend(imgs)
        if sample_img is None and imgs:
            sample_img = imgs[0][0]
    
    print(f"Total training images: {len(train_images)}")
    
    # Get sample dimensions
    orig_width, orig_height = get_image_dimensions(sample_img) if sample_img else (0, 0)
    print(f"Sample image dimensions: {orig_width}x{orig_height}")
    
    # Shuffle and split
    random.seed(42)
    random.shuffle(train_images)
    
    # Create train/val split if no separate test folder
    if test_dir is None and VAL_SPLIT > 0:
        split_idx = int(len(train_images) * (1 - VAL_SPLIT))
        val_images = train_images[split_idx:]
        train_images = train_images[:split_idx]
        print(f"Created validation split: {len(val_images)} images")
    else:
        val_images = []
    
    # Collect test images if available
    test_images = []
    if test_dir:
        print("\\nCollecting test images...")
        for class_name in class_names:
            class_dir = os.path.join(test_dir, class_name)
            if os.path.exists(class_dir):
                test_images.extend(collect_images(class_dir, class_to_idx[class_name]))
        print(f"Total test images: {len(test_images)}")
    
    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    shard_size_bytes = SHARD_SIZE_MB * 1024 * 1024
    all_shard_paths = {'train': [], 'val': [], 'test': []}
    all_checksums = {}
    
    # Write training shards
    print(f"\\nWriting training shards (~{SHARD_SIZE_MB}MB each)...")
    train_shards, train_checksums = write_shards(
        train_images, 
        os.path.join(OUTPUT_DIR, 'train'),
        shard_size_bytes
    )
    all_shard_paths['train'] = train_shards
    all_checksums.update(train_checksums)
    
    # Write validation shards
    if val_images:
        print(f"\\nWriting validation shards...")
        val_shards, val_checksums = write_shards(
            val_images,
            os.path.join(OUTPUT_DIR, 'val'),
            shard_size_bytes
        )
        all_shard_paths['val'] = val_shards
        all_checksums.update(val_checksums)
    
    # Write test shards
    if test_images:
        print(f"\\nWriting test shards...")
        test_shards, test_checksums = write_shards(
            test_images,
            os.path.join(OUTPUT_DIR, 'test'),
            shard_size_bytes
        )
        all_shard_paths['test'] = test_shards
        all_checksums.update(test_checksums)
    
    # Create metadata
    metadata = {
        'numClasses': len(class_names),
        'classNames': class_names,
        'trainSamples': len(train_images),
        'valSamples': len(val_images) if val_images else None,
        'testSamples': len(test_images) if test_images else None,
        'originalWidth': orig_width,
        'originalHeight': orig_height,
        'shardPaths': all_shard_paths,
        'checksums': all_checksums,
        'createdAt': __import__('datetime').datetime.now().isoformat()
    }
    
    metadata_path = os.path.join(OUTPUT_DIR, 'metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\\n{'=' * 60}")
    print("Conversion Complete!")
    print(f"{'=' * 60}")
    print(f"Output directory: {OUTPUT_DIR}")
    print(f"Training shards: {len(all_shard_paths['train'])}")
    if all_shard_paths['val']:
        print(f"Validation shards: {len(all_shard_paths['val'])}")
    if all_shard_paths['test']:
        print(f"Test shards: {len(all_shard_paths['test'])}")
    print(f"Metadata: {metadata_path}")
    
    return metadata

if __name__ == '__main__':
    convert_dataset()
`;
}

/**
 * Generate a minimal requirements.txt for the Cloud Function
 */
export function generateRequirements(): string {
    return `tensorflow>=2.13.0
Pillow>=9.0.0
google-cloud-storage>=2.0.0
google-cloud-firestore>=2.0.0
`;
}
